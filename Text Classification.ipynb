{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 20 NEWSGROUP DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms Used:\n",
    "* Naive Bayes \n",
    "* Linear SVC \n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORING THE ABOVE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test  = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples: 11314\n",
      "Number of Testing Samples: 7532\n",
      "----------------------------------------\n",
      "Class  0 = alt.atheism\n",
      "Class  1 = comp.graphics\n",
      "Class  2 = comp.os.ms-windows.misc\n",
      "Class  3 = comp.sys.ibm.pc.hardware\n",
      "Class  4 = comp.sys.mac.hardware\n",
      "Class  5 = comp.windows.x\n",
      "Class  6 = misc.forsale\n",
      "Class  7 = rec.autos\n",
      "Class  8 = rec.motorcycles\n",
      "Class  9 = rec.sport.baseball\n",
      "Class 10 = rec.sport.hockey\n",
      "Class 11 = sci.crypt\n",
      "Class 12 = sci.electronics\n",
      "Class 13 = sci.med\n",
      "Class 14 = sci.space\n",
      "Class 15 = soc.religion.christian\n",
      "Class 16 = talk.politics.guns\n",
      "Class 17 = talk.politics.mideast\n",
      "Class 18 = talk.politics.misc\n",
      "Class 19 = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Training Samples: {len(train['data'])}\")\n",
    "print(f\"Number of Testing Samples: {len(test['data'])}\")\n",
    "print(40*'-')\n",
    "\n",
    "for idx, label in enumerate(train['target_names']):\n",
    "    print(f'Class {idx:2d} = {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  80.2%\n"
     ]
    }
   ],
   "source": [
    "# Classify text with Naive Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create DTM\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(train['data'])\n",
    "test_dtm = cv.transform(test['data'])\n",
    "\n",
    "# Create Classifier\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm, train['target'])\n",
    "\n",
    "# Predict and display score\n",
    "predicted = nb.predict(test_dtm)\n",
    "scr = 100.0 * nb.score(test_dtm, test['target'])\n",
    "print(f'NB prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  81.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC (TF-IDF with Stop Words) prediction accuracy =  83.1%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C=1000,max_iter=2000)\n",
    "\n",
    "svc = svc.fit(train_dtm_tf, train['target'])\n",
    "predicted = svc.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * svc.score(test_dtm_tf, test['target'])\n",
    "print(f'SVC (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  83.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(train_dtm_tf,train['target'])\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "score = 100 * lr.score(test_dtm_tf,test['target'])\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {score:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'edu', 'god', 'caltech', 'atheists']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'edu', 'image', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'edu', 'file', 'dos', 'files']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'drive', 'ide', 'card', 'edu']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'edu', 'drive', 'quadra']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'server', 'com']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'edu', '00', 'offer', 'shipping']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'com', 'cars', 'edu', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'com', 'dod', 'edu', 'ride']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['edu', 'baseball', 'year', 'team', 'game']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'edu']\n",
      "\n",
      "sci.crypt:\n",
      "['key', 'clipper', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['edu', 'com', 'use', 'lines', 'subject']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'edu', 'geb', 'banks', 'gordon']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'edu', 'henry', 'moon']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'church', 'edu']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'edu', 'guns', 'com', 'people']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['edu', 'com', 'cramer', 'optilink', 'people']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'jesus', 'edu', 'com']\n"
     ]
    }
   ],
   "source": [
    "#Printing top 5 words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING NEW STOP WORDS AND THEN PREDICTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  81.1%\n"
     ]
    }
   ],
   "source": [
    "#get current stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add com, edu and re to stop words\n",
    "stop_words.extend(['com', 'edu', 're'])\n",
    "\n",
    "# Create DTM, use custom defined stop words\n",
    "tf_cv = TfidfVectorizer(stop_words=stop_words)\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', '3d', 'files', 'lines']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'file', 'dos', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'drive', 'ide', 'card', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'drive', 'quadra', 'se']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'server', 'widget']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', '00', 'offer', 'shipping', 'new']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'engine', 'article', 'would']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'ride', 'bikes', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'team', 'game', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['key', 'clipper', 'encryption', 'chip', 'keys']\n",
      "\n",
      "sci.electronics:\n",
      "['use', 'lines', 'one', 'subject', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'msg']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'henry', 'moon', 'alaska']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'church', 'bible']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'would', 'people', 'fbi']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['cramer', 'optilink', 'people', 'clinton', 'writes']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'jesus', 'christian', 'kent']\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  84.9%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, train['target'])\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, test['target'])\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['atheism', 'keith', 'god', 'atheists', 'bible']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', '3d', 'image', 'images', '3do']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'cica', 'file', '13', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'gateway', 'monitor', 'ide', 'pc']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'powerbook', 'duo']\n",
      "\n",
      "comp.windows.x:\n",
      "['motif', 'window', 'server', 'widget', 'x11r5']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'shipping', 'wanted', 'distribution', 'forsale']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'engine', 'toyota', 'dealer']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'bikes', 'motorcycle', 'ride']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'phillies', 'team', 'year', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'nhl', 'team', 'game', 'season']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'chip', 'crypto']\n",
      "\n",
      "sci.electronics:\n",
      "['circuit', 'electronics', 'mhz', 'power', 'radar']\n",
      "\n",
      "sci.med:\n",
      "['doctor', 'msg', 'geb', 'disease', 'banks']\n",
      "\n",
      "sci.space:\n",
      "['space', 'orbit', 'moon', 'launch', 'dc']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'church', 'clh', 'christians', 'christ']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'firearms', 'weapons', 'atf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'turkish', 'armenian', 'jews']\n",
      "\n",
      "talk.politics.misc:\n",
      "['clinton', 'kaldis', 'cramer', 'government', 'optilink']\n",
      "\n",
      "talk.religion.misc:\n",
      "['sandvik', 'christian', 'morality', 'jesus', 'koresh']\n"
     ]
    }
   ],
   "source": [
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(lr.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-GRAMS (For phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'course', 'introduces', 'many', 'concepts', 'in', 'data', 'science', 'this course', 'course introduces', 'introduces many', 'many concepts', 'concepts in', 'in data', 'data science', 'this course introduces', 'course introduces many', 'introduces many concepts', 'many concepts in', 'concepts in data', 'in data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'This course introduces many concepts in data science.'\n",
    "\n",
    "# Tokenize sentance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Analyze sentance\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "# Display n-grams\n",
    "print(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts in\n",
      "2 concepts in data\n",
      "3 course\n",
      "4 course introduces\n",
      "5 course introduces many\n",
      "6 data\n",
      "7 data science\n",
      "8 in\n",
      "9 in data\n",
      "10 in data science\n",
      "11 introduces\n",
      "12 introduces many\n",
      "13 introduces many concepts\n",
      "14 many\n",
      "15 many concepts\n",
      "16 many concepts in\n",
      "17 science\n",
      "18 this\n",
      "19 this course\n",
      "20 this course introduces\n",
      "----------------------------------------\n",
      "['This course is data science!']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentence\n",
    "cv = cv.fit([my_text])\n",
    "\n",
    "# Sort tokens\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Display token mapping\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "# Display new sentence\n",
    "print(40*'-')\n",
    "out_list = ['This course is data science!']\n",
    "\n",
    "# Transform new sentence to original sentence DTM\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "\n",
    "# Display count vector indices for new sentance tokens\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with ngram_range 1-2) prediction accuracy =  80.7%\n"
     ]
    }
   ],
   "source": [
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with ngram_range 1-2) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n",
      "Number of Negative Reviews: 1000\n",
      "Number of Positive Reviews: 1000\n",
      "Sample Review:\n",
      "Label: 0\n",
      "not a great twelve months for either of the principals from this movie . earlier this year , nora ephron wrote and produced one of the year ' s least likeable \" comedies \" called hanging up , featuring a bunch of annoying women ( ironically , lisa kudrow played one in that film as well ) who barely have time to care about anyone but themselves . ick . . . real sweet stuff . but her little unsuccessful project was nothing compared to what john travolta went through earlier this summer , with a film entitled battlefield earth . i seemed to be the only person on this planet who somehow appreciated the film , even if it was on a \" cheese \" factor , as everybody , and i mean everybody else , pretty much classified the movie as one of the worst disasters of all time . yipes . . . another beauty . so what happens when you put these two people in the same room and come out with a movie co - starring the ever - versatile lisa kudrow ? uhhhm , you guessed it . . . not much . plot : in order to escape major financial difficulties , a local weatherman hooks up with his ball - picking lotto girlfriend and rigs the state lottery . but as more and more people find out about their scheme , more and more people demand a part of their winnings , and more and more problems arise . critique : simply stated , i didn ' t laugh once during this entire picture . for a comedy , it offered me a few smiles , a bunch of nincompoops as characters , a miscast john travolta hamming it up and lisa kudrow , in what can only described as a \" sluttier \" version of her character of phoebe on tv ' s \" friends \" . this film was not as disastrous as i thought it would be , but it was pretty close . thankfully , the clips of travolta dressed up in goofy outfits , dancing as the weatherman were left in the film ' s trailer , and not in this final cut . and not unlike the worst movie of the year so far , beautiful , this film also managed to feature many unsympathetic , idiotic and just plain irritating characters in its cast . foremost was travolta ' s character , who declined to give us any reason to care for him once in the entire movie . and for me , the casting choice of john travolta for this role was just plain wrong . he didn ' t fit the part . i just saw him in get shorty the other day and thought about how perfect he was for that role . a cool , calculating roughneck with a certain hip , suave \" je ne sais quoi \" . in this film , he looks like he ' s trying to be funny , trying to be bad , trying to be good . we ' re not supposed to be able to notice that , and when we do , at least in my case , i consider it a wrong choice in casting . add that to lisa kudrow , boring us with yet another one of her patented \" dumb blonde \" routines , but this time , dressed in sexier outfits . michael rapaport , stretching one small acting muscle to play the guy who isn ' t quite up at the same speed level as everyone else . and a truckload of empty comedic bullet shells , and you ' ve got yourself an extremely quiet audience anticipating punch lines that never quite materialize . the only real good thing that i could say about this film is that its story was actually half - interesting and never really bored me . i also liked michael moore ' s perverted cousin character , and i loved , and i say it again , loved the character that bill pullman played . give this dude his own movie ! he played a lazy cop , a man who tries everything not to do any real work . he fakes injuries to get off duty , tries to avoid arrest situations so that he won ' t have to fill out any forms . . . now there ' s a base of humor . sadly , the filmmakers decided to bring him into play with only about half an hour left in the film . and there ' s not much else i can say about this movie , folks . on the whole , it was lame , included a slew of unlikable characters fiddling around in a pool of unfunny lines , and very little of interest for any target audience . but get somebody to write up a movie featuring that lazy cop played by pullman and i ' m there !\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import movie_reviews\n",
    "#load movie reviews, each review is a list of words\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "print (\"Number of Reviews:\", len(documents))\n",
    "#shuffle reviews to mix negative and positive reviews\n",
    "#set random seed for reproducibility\n",
    "random.seed(23)\n",
    "random.shuffle(documents)\n",
    "\n",
    "#list to store all review text\n",
    "text_data = []\n",
    "#label\n",
    "label = []\n",
    "for i in range(len(documents)):\n",
    "    #join list of words to create a review and add to text_data\n",
    "    text_data.append(' '.join(documents[i][0]))\n",
    "    #map neg to 0, pos to 1 and add to label\n",
    "    label.append(0 if documents[i][1]=='neg' else 1)\n",
    "\n",
    "print(\"Number of Negative Reviews:\", label.count(0))\n",
    "print(\"Number of Positive Reviews:\", label.count(1))    \n",
    "\n",
    "#split to train and text\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(text_data, label, test_size=0.25, random_state=23)\n",
    "\n",
    "#print one example review in the training text set\n",
    "print(\"Sample Review:\")\n",
    "print('Label:', y_train[0])\n",
    "print(mvr_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with stop words) prediction accuracy =  78.6%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.80      0.79       256\n",
      "    Positive       0.79      0.77      0.78       244\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.79      0.79      0.79       500\n",
      "weighted avg       0.79      0.79      0.79       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes pipeline to classify\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "# Fit model, predict, and display results\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, y_train)\n",
    "y_pred = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, y_test)\n",
    "print(f'NB (TF-IDF with stop words) prediction accuracy = {scr:5.1f}%')\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Words of Positive Reviews:\n",
      "['film', 'movie', 'like', 'story', 'life', 'good', 'just', 'time', 'character', 'characters', 'films', 'great', 'best', 'does', 'way', 'love', 'really', 'people', 'man', 'little']\n"
     ]
    }
   ],
   "source": [
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "top_word_index = np.argsort(nb.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  82.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Words of Positive Reviews:\n",
      "['great', 'fun', 'life', 'hilarious', 'memorable', 'overall', 'quite', 'different', 'good', 'terrific', 'especially', 'trek', 'works', 'seen', 'performances', 'perfect', 'perfectly', 'comic', 'town', 'gives']\n"
     ]
    }
   ],
   "source": [
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 words of Negative Reviews:\n",
      "['bad', 'worst', 'plot', 'supposed', 'unfortunately', 'harry', 'boring', 'script', 'stupid', 'reason', 'poor', 'awful', 'waste', 'cheap', 'attempt', 'jakob', 'dull', 'lame', 'looks', 'better']\n"
     ]
    }
   ],
   "source": [
    "#reverse label value so that negative reviews have label 1\n",
    "y_train_reverse = [0 if y==1 else 1 for y in y_train]\n",
    "lr = lr.fit(train_dtm_tf, y_train_reverse)\n",
    "\n",
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 words of Negative Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Palla Anuraag Sharma/nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Palla Anuraag Sharma\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-c75db7956428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# use custom tokenize when creating vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtf_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_dtm_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvr_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mtest_dtm_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvr_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \"\"\"\n\u001b[0;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1199\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-c75db7956428>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Define function to tokenize text and apply stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Palla Anuraag Sharma/nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Palla Anuraag Sharma\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Define function to tokenize text and apply stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "# use custom tokenize when creating vectorizer\n",
    "tf_cv = TfidfVectorizer(tokenizer=tokenize)\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stemming) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
